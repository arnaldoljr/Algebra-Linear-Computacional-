{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large{ \\textit{UNIVERSIDADE FEDERAL DO CEARÁ  } }$$\n",
    "\n",
    "$$ \\large{\\textit{Departamento de Ciências da Computação }} $$\n",
    "\n",
    "\n",
    "$$ \\large{\\textit{Mestrado e Doutorado em Ciências da Computação -  MDCC }} $$\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large{{\\textbf{ DISCIPLINA DE ÁLGEBRA LINEAR COMPUTACIONAL}}}$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\large{\\texttt{Docente: Professor Dr. Creto Vidal}}$$\n",
    "\n",
    "$$\\large{\\texttt{Discente: Arnaldo Araújo}}$$\n",
    "<hr/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método do Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Método Interativo** para solução de Sistemas Lineares particulares(Matrizes simétricas e positiva definidas)\n",
    "\n",
    "* È motividado a utilização deste método para matrizes esparsa (cheia de zeros). Visto que fatorar matrizes esparsas tende a gerar graves problemas.\n",
    "\n",
    "* O método mostra-se promissor por  minimizar a norma do vetor erro em $Ax=b$ em subspaços vetoriais específicos\n",
    "\n",
    "* Dado um sistema $Ax = b$ e um ponto para a aproximação inicial $x^{0}$ da solução do sistema, queremos reduzir o resíduo dado por $r^{0} = b − Ax^{0}$ até que este resíduo seja nulo\n",
    "\n",
    "* Para que o resíduo diminua, tomamos uma direção v e corrigimos $x^{0}$ nesta direção, ou seja,$x^{1}= x^{0}+ \\lambda v$ de forma que $r^{1} = b − Ax^{1} < r^{0}$\n",
    "\n",
    "* De forma que a medida que o algoritmo evolui o resíduo tende a zero\n",
    "\n",
    "* Vamos no sentido contrário ao gradiente para buscar esse ponto de mínimo onde o resíduo será o menor possível. -> **$ \\nabla f(x^{(i)}) = b -Ax^{(i)} = r^{(i)}  $**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PseudoCódigo - *Gradiente Descendente * **\n",
    "\n",
    "*GradienteDesc(A,b,$x^{(0)}$,error):*\n",
    "\n",
    "> i ← 0\n",
    "\n",
    "> x ← x(0)\n",
    "\n",
    "> r ← b − Ax\n",
    " \n",
    "> while ||r|| > error\n",
    ">> do\n",
    ">>> $\\lambda$ ← $r^{T}r/r^{T}Ar$\n",
    "\n",
    ">>> x ← x + $\\lambda$r\n",
    "\n",
    ">>> r ← b − Ax\n",
    "\n",
    ">>>i ← i + 1\n",
    "\n",
    "> return x, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradienteDesc(A,b,x0,erro):\n",
    "    step = 0\n",
    "    x = np.copy(x0)\n",
    "    r = b - (np.dot(A,x)) #vetor de resíduo(guarda o erro)\n",
    "    while LA.norm(r)>erro:\n",
    "        Lambda = (np.dot(r.T,r)/(np.dot(np.dot(r.T,A),r)))\n",
    "        x = x + Lambda*r\n",
    "        r = -((np.dot(A,x))-b)\n",
    "        step = step+1\n",
    "    return(x,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.random.rand(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22122797,  0.19120186,  0.94628918])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 12.,   3.,  -5.],\n",
       "       [  1.,   5.,   3.],\n",
       "       [  3.,   7.,  13.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[ 12., 3., -5.],\n",
    " [ 1., 5., 3.],[3.,7.,13]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,  28.,  76.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array([1.,28.,76.])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.00006611,  2.99984951,  4.00009416]), 22)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradienteDesc(A,b,x0,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método do Gradiente Conjugado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Método Interativo** para solução de Sistemas Lineares particulares(Matrizes simétricas e positiva definidas)\n",
    "\n",
    "* Uma variação do *Método do Gradiente Descendente*\n",
    "\n",
    "* È motividado a utilização deste método para matrizes esparsa (cheia de zeros). Visto que fatorar matrizes esparsas tende a gerar graves problemas.\n",
    "\n",
    "* O método mostra-se promissor por  minimizar a norma do vetor erro em $Ax=b$ em subspaços vetoriais específicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PseudoCódigo**\n",
    "\n",
    "*GradienteConj(A,b,$x^{(0)}$,error):*\n",
    "\n",
    "> i ← 0\n",
    "\n",
    "> x ← x(0)\n",
    "\n",
    "> r ← b − Ax\n",
    "\n",
    "> s ← r\n",
    " \n",
    "> while ||r|| > error\n",
    ">> do\n",
    ">>> $\\lambda$ ← $s^{T}r/s^{T}As$\n",
    "\n",
    ">>> x ← x + $\\lambda$s\n",
    "\n",
    ">>> r ← b − Ax\n",
    "\n",
    ">>> β ← $−(r^{T}As/s^{T}As)$\n",
    "\n",
    ">>> s ← r + βs\n",
    "\n",
    ">>>i ← i + 1\n",
    "\n",
    "> return x, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realizando os imports para todo o projeto\n",
    "import numpy as np\n",
    "from scipy import linalg as LA ## testes\n",
    "from __future__ import print_function \n",
    "from pprint import pprint\n",
    "from math import sqrt\n",
    "from scipy import random, linalg, dot, diag, all, allclose\n",
    "from __future__ import print_function \n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 12.,   3.,  -5.],\n",
       "       [  1.,   5.,   3.],\n",
       "       [  3.,   7.,  13.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testes\n",
    "A = np.array([[ 12., 3., -5.],\n",
    " [ 1., 5., 3.],[3.,7.,13]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,  28.,  76.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array([1.,28.,76.])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22122797,  0.19120186,  0.94628918])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.random.rand(3)\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "erro = 0.001\n",
    "step = 0\n",
    "x = np.copy(x0)\n",
    "r = b - (np.dot(A,x)) #vetor de resíduo(guarda o erro)\n",
    "s = np.copy(r)                #gradiente recebe o erro\n",
    "while LA.norm(r)>erro:\n",
    "    Lambda = (np.dot(r.T,s)/(np.dot(np.dot(s.T,A),s)))\n",
    "    x = x + Lambda*s\n",
    "    r = -((np.dot(A,x))-b)\n",
    "    beta = -((np.dot((np.dot(r.T,A)),s))/(np.dot(np.dot(s.T,A),s)))\n",
    "    s = r + beta*s\n",
    "    step = step+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22122797,  0.19120186,  0.94628918])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 12.,   3.,  -5.],\n",
       "       [  1.,   5.,   3.],\n",
       "       [  3.,   7.,  13.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,  28.,  76.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erro = 0.001\n",
    "erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradienteConj(A,b,x0,erro):\n",
    "    step = 0\n",
    "    x = np.copy(x0)\n",
    "    r = b - (np.dot(A,x)) #vetor de resíduo(guarda o erro)\n",
    "    s = np.copy(r)                #gradiente recebe o erro\n",
    "    while LA.norm(r)>erro:\n",
    "        Lambda = (np.dot(r.T,s)/(np.dot(np.dot(s.T,A),s)))\n",
    "        x = x + Lambda*s\n",
    "        r = -((np.dot(A,x))-b)\n",
    "        beta = -((np.dot((np.dot(r.T,A)),s))/(np.dot(np.dot(s.T,A),s)))\n",
    "        s = r + beta*s\n",
    "        step = step+1\n",
    "    return(x,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.99997443,  2.99991578,  4.00005281]), 20)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradienteConj(A,b,x0,erro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
